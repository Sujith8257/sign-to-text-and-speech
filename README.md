# Indian Sign Language Recognition System

A comprehensive real-time **Indian Sign Language (ISL)** recognition system. The system uses computer vision, machine learning, and web technologies to provide real-time gesture recognition through a web interface. It supports both single-hand and two-hand gesture recognition with high accuracy.

## ğŸ“¸ Dataset Overview

The system recognizes **35 Indian Sign Language classes** covering digits (1-9) and letters (A-Z). Below is a visual representation of all classes in the dataset:

<div align="center">

![Indian Sign Language Dataset - All 35 Classes](dataset_classes_visualization.png)

*Figure 1: Complete visualization of all 35 classes in the Indian Sign Language dataset. Each cell shows a sample image from the corresponding class folder.*

</div>

## ğŸ“‹ Table of Contents

- [Features](#features)
- [Architecture](#architecture)
- [System Requirements](#system-requirements)
- [Installation](#installation)
- [Dataset](#dataset)
- [Model Architecture](#model-architecture)
- [Usage](#usage)
- [Project Structure](#project-structure)
- [API Documentation](#api-documentation)
- [Training](#training)
- [Performance](#performance)
- [Troubleshooting](#troubleshooting)
- [Contributing](#contributing)
- [License](#license)

## âœ¨ Features

### ISL Model Support
- **ISL Landmark Model**: Keras neural network using MediaPipe hand landmarks (35 classes: digits 1-9 + letters A-Z)
- **ISL Skeleton Model**: RandomForest classifier using normalized landmark features (35 classes)
- **Two-Hand Support**: Recognizes gestures from both single hand and two hands simultaneously

### Real-Time Recognition
- Live webcam feed processing
- Real-time gesture detection and classification
- WebSocket-based communication for instant updates
- Gesture stability detection to reduce false positives

### User Interface
- Modern web-based interface
- Real-time video streaming (MJPEG)
- Live prediction display
- Two-hand detection visualization
- Confidence score visualization

### Technical Features
- GPU acceleration support (CUDA)
- Multi-backend camera support (DirectShow, MSMF, V4L2)
- Robust error handling and camera reconnection
- Cross-platform compatibility (Windows, Linux, macOS)

## ğŸ—ï¸ Architecture

### System Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Web Browser (Client)                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚   HTML/JS    â”‚  â”‚  WebSocket   â”‚  â”‚   MJPEG      â”‚     â”‚
â”‚  â”‚   Interface  â”‚â—„â”€â”¤   Socket.IO  â”‚â—„â”€â”¤   Stream     â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â–²
                            â”‚ HTTP/WebSocket
                            â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Flask Application Server (app.py)               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚              Video Frame Generator                   â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚   â”‚
â”‚  â”‚  â”‚   Camera     â”‚  â”‚  MediaPipe   â”‚                â”‚   â”‚
â”‚  â”‚  â”‚   Capture    â”‚â”€â–ºâ”‚  Hand        â”‚                â”‚   â”‚
â”‚  â”‚  â”‚              â”‚  â”‚  Detection   â”‚                â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚   â”‚
â”‚  â”‚                          â”‚                           â”‚   â”‚
â”‚  â”‚                          â–¼                           â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚   â”‚
â”‚  â”‚  â”‚      ISL Model Prediction             â”‚          â”‚   â”‚
â”‚  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚          â”‚   â”‚
â”‚  â”‚  â”‚  â”‚   ISL     â”‚      â”‚   ISL     â”‚      â”‚          â”‚   â”‚
â”‚  â”‚  â”‚  â”‚ Keras     â”‚      â”‚ Skeleton â”‚      â”‚          â”‚   â”‚
â”‚  â”‚  â”‚  â”‚ Landmark  â”‚      â”‚ Random   â”‚      â”‚          â”‚   â”‚
â”‚  â”‚  â”‚  â”‚ Model     â”‚      â”‚ Forest   â”‚      â”‚          â”‚   â”‚
â”‚  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚          â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Component Breakdown

#### 1. **Frontend (Web Interface)**
- **Technology**: HTML5, JavaScript, Socket.IO Client
- **Features**:
  - Real-time video display
  - Live prediction updates
  - Model switching controls
  - Responsive design

#### 2. **Backend Server (Flask)**
- **Technology**: Flask, Flask-SocketIO
- **Responsibilities**:
  - HTTP server for web interface
  - WebSocket server for real-time communication
  - MJPEG video streaming
  - Request routing and API endpoints

#### 3. **Computer Vision Pipeline**
- **MediaPipe Hands**: Hand landmark detection
- **OpenCV**: Image processing and camera management
- **Frame Processing**: Real-time frame capture and preprocessing

#### 4. **Machine Learning Models**

##### ISL Keras Landmark Model
- **Type**: Dense Neural Network (Keras/TensorFlow)
- **Input**: 84 normalized hand landmark features (42 per hand Ã— 2 hands, padded with zeros if single hand)
- **Output**: 35 classes (digits 1-9 + letters A-Z)
- **Architecture**:
  ```
  Input (84 features)
    â†“
  Dense(256) + BatchNorm + Dropout(0.3)
    â†“
  Dense(128) + BatchNorm + Dropout(0.3)
    â†“
  Dense(64) + BatchNorm + Dropout(0.2)
    â†“
  Dense(35, softmax) â†’ Output
  ```
- **File**: `model/indian_sign_model.h5` or `checkpoints/best_model_*.h5`
- **Accuracy**: 99.88% test accuracy
- **Preprocessing**:
  - Extract MediaPipe hand landmarks from all detected hands
  - Normalize coordinates relative to hand bounding box
  - Combine features from multiple hands (pad with zeros if single hand)
  - Create feature vector: 84 features (42 per hand)

##### ISL Skeleton Model (RandomForest)
- **Type**: RandomForest Classifier (scikit-learn)
- **Input**: 84 normalized hand landmark features (same format as Keras model)
- **Output**: 35 classes (digits 1-9 + letters A-Z)
- **File**: `model/model.p` (pickled scikit-learn model)
- **Preprocessing**:
  - Extract MediaPipe hand landmarks
  - Normalize coordinates relative to hand bounding box
  - Combine features from multiple hands (pad with zeros if single hand)
  - Create feature vector: 84 features

#### 5. **Prediction Pipeline**

```
Camera Frame
    â†“
MediaPipe Hand Detection (max 2 hands)
    â†“
Extract Landmarks from All Detected Hands
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Feature Combination                â”‚
â”‚   - Single hand: 42 features +       â”‚
â”‚     42 zeros = 84 features          â”‚
â”‚   - Two hands: 42 + 42 = 84 featuresâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   ISL Keras     â”‚   ISL Skeleton  â”‚
â”‚   Landmark      â”‚   RandomForest  â”‚
â”‚   Model         â”‚   Model         â”‚
â”‚   (84 features) â”‚   (84 features) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
Ensemble Prediction (use best confidence)
    â†“
WebSocket Emission
    â†“
Frontend Display
```

## ğŸ’» System Requirements

### Minimum Requirements
- **OS**: Windows 10/11, Linux (Ubuntu 18.04+), macOS 10.14+
- **Python**: 3.8 or higher
- **RAM**: 4 GB minimum (8 GB recommended)
- **Storage**: 2 GB free space
- **Camera**: USB webcam or built-in camera

### Recommended Requirements
- **CPU**: Multi-core processor (Intel i5/AMD Ryzen 5 or better)
- **GPU**: NVIDIA GPU with CUDA support (optional, for faster inference)
- **RAM**: 8 GB or more
- **Camera**: HD webcam (720p or higher)

### Software Dependencies
- Python 3.8+
- pip (Python package manager)
- Git (for cloning repository)

## ğŸ“¦ Installation

#### Step 1: Clone the Repository

```bash
git clone https://github.com/Sujith8257/sign-to-text-and-speech.git
cd sign-to-text-and-speech
```

### Step 2: Create Virtual Environment

**Windows:**
```bash
python -m venv venv
venv\Scripts\activate
```

**Linux/macOS:**
```bash
python3 -m venv venv
source venv/bin/activate
```

### Step 3: Install Dependencies

```bash
pip install -r requirements.txt
```

### Step 4: Verify Installation

```bash
python -c "import cv2, mediapipe, flask, tensorflow; print('All dependencies installed successfully!')"
```

## ğŸ“Š Dataset

### Indian Sign Language Dataset

The system includes a dataset of **35 classes** covering:
- **Digits**: 1, 2, 3, 4, 5, 6, 7, 8, 9
- **Letters**: A through Z (26 letters)

**Dataset Structure:**
```
Indian/
â”œâ”€â”€ 1/
â”‚   â”œâ”€â”€ 1.jpg
â”‚   â”œâ”€â”€ 2.jpg
â”‚   â””â”€â”€ ...
â”œâ”€â”€ 2/
â”‚   â”œâ”€â”€ 1.jpg
â”‚   â””â”€â”€ ...
â”œâ”€â”€ A/
â”‚   â”œâ”€â”€ 1.jpg
â”‚   â””â”€â”€ ...
â””â”€â”€ ... (35 total classes)
```

**Dataset Statistics:**
- **Total Classes**: 35
- **Total Images**: ~42,510 images
- **Format**: JPG images
- **Organization**: One folder per class

**Visualization:**
The dataset visualization image (shown at the top of this README) displays a grid of sample images from all 35 classes, providing a quick overview of the sign language gestures the system can recognize.

## ğŸ§  Model Architecture

### ISL Keras Landmark Model

**Input Features:**
- Hand landmarks from MediaPipe (21 landmarks per hand)
- Each landmark has (x, y) coordinates normalized relative to hand bounding box
- Single hand: 42 features + 42 zeros = 84 features
- Two hands: 42 features (hand 1) + 42 features (hand 2) = 84 features

**Model Architecture:**

| Layer | Type | Units | Output Shape | Parameters |
|-------|------|-------|--------------|------------|
| Input | InputLayer | - | (84) | 0 |
| Dense1 | Dense | 256 | (256) | 21,760 |
| | BatchNorm | - | (256) | 1,024 |
| | Dropout(0.3) | - | (256) | 0 |
| Dense2 | Dense | 128 | (128) | 32,896 |
| | BatchNorm | - | (128) | 512 |
| | Dropout(0.3) | - | (128) | 0 |
| Dense3 | Dense | 64 | (64) | 8,256 |
| | BatchNorm | - | (64) | 256 |
| | Dropout(0.2) | - | (64) | 0 |
| Output | Dense | 35 | (35) | 2,275 |

**Total Parameters**: 66,979 trainable parameters

**Model Details:**
- **Algorithm**: Dense Neural Network (Keras/TensorFlow)
- **Classes**: 35 (digits 1-9 + letters A-Z)
- **Training**: Uses MediaPipe landmark data from images
- **Test Accuracy**: 99.88%
- **Validation Accuracy**: 100.00%
- **Inference Speed**: ~2-5 ms per prediction (CPU)

### ISL Skeleton Model (RandomForest)

**Input Features:**
- Same 84-feature format as Keras model
- Normalized landmark coordinates

**Model Details:**
- **Algorithm**: RandomForest Classifier (scikit-learn)
- **Classes**: 35 (digits 1-9 + letters A-Z)
- **Training**: Uses MediaPipe landmark data
- **Inference Speed**: ~1-2 ms per prediction
- **File**: `model/model.p`

**Training Configuration:**
- **Optimizer**: Adam (learning rate: 0.001)
- **Loss Function**: Categorical Crossentropy
- **Batch Size**: 64
- **Epochs**: 100 (with early stopping, patience=15)
- **Validation Split**: 20%
- **Test Split**: 10%
- **Checkpointing**: Best model saved every epoch, periodic checkpoints every 5 epochs

## ğŸš€ Usage

### Starting the Application

1. **Activate virtual environment** (if not already active):
   ```bash
   # Windows
   venv\Scripts\activate
   
   # Linux/macOS
   source venv/bin/activate
   ```

2. **Run the Flask application**:
   ```bash
   python app.py
   ```

3. **Open web browser** and navigate to:
   ```
   http://localhost:5000
   ```

### Using the Web Interface

1. **Allow camera access** when prompted by the browser
2. **Position your hand** in front of the camera
3. **Make sign language gestures** - predictions will appear in real-time
4. **Switch models** using the model selector (if both models are available)
5. **View confidence scores** displayed with each prediction

### Model Usage

The system uses an ensemble approach:
- Both ISL Keras and ISL Skeleton models run predictions simultaneously
- The prediction with higher confidence is selected
- If both models agree, confidence is boosted
- Supports both single-hand and two-hand gestures

### Command Line Options

Currently, the application runs with default settings. Future versions may include:
- Custom port selection
- Camera index selection
- Model path specification
- Debug mode

## ğŸ“ Project Structure

```
sign-to-text-and-speech/
â”‚
â”œâ”€â”€ app.py                          # Main Flask application
â”œâ”€â”€ train_indian_model.py           # ISL CNN training script
â”œâ”€â”€ inference_indian.py             # Standalone inference script
â”œâ”€â”€ generate_dataset_visualization.py # Dataset visualization generator
â”‚
â”œâ”€â”€ requirements.txt                # Python dependencies
â”œâ”€â”€ requirements_training.txt       # Additional training dependencies
â”‚
â”œâ”€â”€ model/                          # Model files directory
â”‚   â”œâ”€â”€ indian_sign_model.h5       # ISL Keras landmark model
â”‚   â”œâ”€â”€ model.p                    # ISL Skeleton RandomForest model
â”‚   â””â”€â”€ model_metadata.json       # Model metadata
â”‚
â”œâ”€â”€ checkpoints/                    # Training checkpoints
â”‚   â”œâ”€â”€ best_model_*.h5            # Best model checkpoints
â”‚   â””â”€â”€ training_history_*.png     # Training history plots
â”‚
â”œâ”€â”€ Indian/                         # ISL Dataset
â”‚   â”œâ”€â”€ 1/                         # Class folders
â”‚   â”œâ”€â”€ 2/
â”‚   â”œâ”€â”€ ...
â”‚   â”œâ”€â”€ A/
â”‚   â”œâ”€â”€ B/
â”‚   â””â”€â”€ ... (35 classes total)
â”‚
â”œâ”€â”€ templates/                     # Web templates
â”‚   â””â”€â”€ index.html                 # Main web interface
â”‚
â”œâ”€â”€ dataset_classes_visualization.png  # Dataset visualization
â”‚
â””â”€â”€ README.md                       # This file
```

## ğŸ”Œ API Documentation

### WebSocket Events

#### Server â†’ Client

**`prediction`**
- **Purpose**: Send real-time prediction
- **Payload**:
  ```json
  {
    "text": "A",
    "confidence": 0.95,
    "model": "ISL-Keras" | "ISL-Skeleton",
    "num_hands": 1 | 2
  }
  ```

### HTTP Endpoints

**`GET /`**
- **Purpose**: Serve main web interface
- **Response**: HTML page

**`GET /video_feed`**
- **Purpose**: MJPEG video stream
- **Response**: Multipart MJPEG stream
- **Content-Type**: `multipart/x-mixed-replace; boundary=frame`

**`GET /api/status`**
- **Purpose**: Get model status
- **Response**:
  ```json
  {
    "skeleton_model": true | false,
    "keras_model": true | false,
    "tensorflow": true | false
  }
  ```

## ğŸ“ Training

### Training the ISL Model

1. **Prepare Dataset**:
   - Ensure `Indian/` folder contains organized class folders
   - Each class folder should contain JPG images
   - Images will be processed to extract MediaPipe hand landmarks

2. **Configure Training**:
   - Edit `train_indian_model.py` to adjust:
     - Dataset path (automatically detected from common locations)
     - Model save paths (defaults to `model/` folder)
     - Batch size (default: 64), epochs (default: 100), learning rate (default: 0.001)
     - Two-hand support (default: True - uses 84 features)

3. **Run Training**:
   ```bash
   # Activate virtual environment first
   venv\Scripts\activate  # Windows
   # or
   source venv/bin/activate  # Linux/macOS
   
   python train_indian_model.py
   ```

4. **Monitor Training**:
   - Check console output for progress
   - Training automatically saves:
     - Best model: `checkpoints/best_model_*.h5`
     - Periodic checkpoints: `checkpoints/checkpoint_epoch_*.h5` (every 5 epochs)
     - Training history plot: `checkpoints/training_history_*.png`
   - Early stopping with patience=15 epochs
   - Learning rate reduction on plateau

5. **Use Trained Model**:
   - Final model saved to `model/indian_sign_model.h5`
   - Skeleton model saved to `model/model.p`
   - Metadata saved to `model/model_metadata.json`
   - Models are automatically loaded by `app.py`

### Training Parameters

**Default Settings:**
- **Batch Size**: 64
- **Epochs**: 100 (with early stopping, patience=15)
- **Learning Rate**: 0.001 (with ReduceLROnPlateau)
- **Input Features**: 84 (42 per hand Ã— 2, padded if single hand)
- **Validation Split**: 20%
- **Test Split**: 10%
- **Two-Hand Support**: Enabled

**Checkpointing:**
- Best model saved every epoch (based on validation accuracy)
- Periodic checkpoints every 5 epochs
- Weight averaging from multiple checkpoints (optional)
- Automatic resume from latest checkpoint

**Expected Performance:**
- **Training Accuracy**: ~99.96%
- **Validation Accuracy**: ~100.00%
- **Test Accuracy**: ~99.88%
- **Top-3 Accuracy**: 100.00%

## ğŸ“ˆ Performance

### Model Performance

**ISL Keras Landmark Model:**
- **Test Accuracy**: 99.88%
- **Validation Accuracy**: 100.00%
- **Training Accuracy**: 99.96%
- **Top-3 Accuracy**: 100.00%
- **Inference Time**: 2-5 ms per frame (CPU)
- **Memory Usage**: ~260 KB (model) + ~50 MB (runtime)

**ISL Skeleton Model (RandomForest):**
- **Accuracy**: High accuracy (ensemble with Keras model)
- **Inference Time**: 1-2 ms per frame
- **Memory Usage**: ~5 MB

**Combined Ensemble:**
- Uses both models and selects prediction with highest confidence
- Typically achieves 99%+ accuracy in real-world scenarios

### System Performance

- **Frame Rate**: 15-30 FPS (depending on hardware)
- **Latency**: <100 ms end-to-end
- **CPU Usage**: 20-40% (single core)
- **GPU Usage**: 30-60% (if GPU available)

### Optimization Tips

1. **Use GPU**: Significantly faster inference for CNN model
2. **Reduce Image Size**: Lower resolution = faster processing
3. **Skip Frames**: Process every Nth frame for lower CPU usage
4. **Batch Processing**: Process multiple predictions together

## ğŸ”§ Troubleshooting

### Camera Issues

**Problem**: Camera not detected
- **Solution**: Check camera permissions in browser/system settings
- **Alternative**: Try different camera index in code (0, 1, 2)

**Problem**: Camera opens but no frames
- **Solution**: Try different backend (DirectShow, MSMF, V4L2)
- **Check**: Camera is not being used by another application

### Model Loading Issues

**Problem**: Model file not found
- **Solution**: Ensure model files exist in project directory
- **Check**: File paths in `app.py` are correct

**Problem**: TensorFlow/Keras errors
- **Solution**: Reinstall TensorFlow: `pip install --upgrade tensorflow`
- **Check**: Python version compatibility (3.8+)

### Performance Issues

**Problem**: Low frame rate
- **Solution**: Reduce image resolution or skip frames
- **Alternative**: Use GPU acceleration if available

**Problem**: High CPU usage
- **Solution**: Reduce batch size or use GPU
- **Check**: Close other applications using CPU

### Web Interface Issues

**Problem**: Video stream not loading
- **Solution**: Check browser console for errors
- **Check**: Flask server is running and accessible

**Problem**: Predictions not updating
- **Solution**: Check WebSocket connection in browser console
- **Check**: Model is loaded correctly (check server logs)

## ğŸ¤ Contributing

Contributions are welcome! Please follow these steps:

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit your changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to the branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

### Areas for Contribution

- Additional sign language support (BSL, etc.)
- Model improvements and optimizations
- UI/UX enhancements
- Documentation improvements
- Bug fixes and performance optimizations
- Support for more complex gestures and phrases

## ğŸ“„ License

This project is licensed under the MIT License - see the LICENSE file for details.

## ğŸ™ Acknowledgments

- **MediaPipe**: For hand landmark detection
- **TensorFlow/Keras**: For deep learning framework
- **Flask**: For web framework
- **OpenCV**: For computer vision operations
- **scikit-learn**: For machine learning utilities

## ğŸ“§ Contact

For questions, issues, or contributions, please open an issue on GitHub.

---

**Last Updated**: December 2025
**Version**: 2.0.0

## ğŸ“ Recent Updates

- **Removed ASL support** - Focused exclusively on Indian Sign Language (ISL)
- **New landmark-based model** - Replaced CNN with efficient landmark-based neural network
- **Two-hand support** - Full support for detecting and recognizing two-hand gestures
- **Improved accuracy** - Achieved 99.88% test accuracy with the new model
- **Model organization** - Models now organized in `model/` folder
- **Checkpoint system** - Enhanced checkpointing with weight averaging support
